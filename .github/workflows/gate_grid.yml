name: gate-grid

on:
  workflow_dispatch:
    inputs:
      profit_target:
        description: "profit target (e.g. 0.10)"
        required: true
        default: "0.10"
      max_days:
        description: "max holding days (e.g. 40)"
        required: true
        default: "40"
      stop_level:
        description: "stop level (e.g. -0.10)"
        required: true
        default: "-0.10"
      max_extend_days:
        description: "extend horizon tag + strategy label horizon add (e.g. 30)"
        required: true
        default: "30"
      p_tail_thresholds:
        description: "comma-separated p_tail thresholds (e.g. 0.15,0.25,0.35)"
        required: true
        default: "0.20,0.30"
      utility_quantiles:
        description: "comma-separated utility quantiles (e.g. 0.60,0.75,0.90)"
        required: true
        default: "0.75,0.90"
      rank_metrics:
        description: "comma-separated rank metric (utility|ret_score|p_success)"
        required: true
        default: "utility"
      lambda_tail:
        description: "utility tail penalty lambda (e.g. 0.05)"
        required: true
        default: "0.05"
      rebuild_all:
        description: "true to rebuild prices/features/labels/models from scratch"
        required: true
        default: "false"

  schedule:
    # 매일 한국시간 08:10 (UTC 23:10)
    - cron: "10 23 * * *"

jobs:
  grid:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    # ✅ schedule에서도 inputs가 비어있을 수 있으니 기본값 fallback
    env:
      PROFIT_TARGET: ${{ github.event.inputs.profit_target || '0.10' }}
      MAX_DAYS: ${{ github.event.inputs.max_days || '40' }}
      STOP_LEVEL: ${{ github.event.inputs.stop_level || '-0.10' }}
      MAX_EXTEND_DAYS: ${{ github.event.inputs.max_extend_days || '30' }}

      P_TAIL_THRESHOLDS: ${{ github.event.inputs.p_tail_thresholds || '0.20,0.30' }}
      UTILITY_QUANTILES: ${{ github.event.inputs.utility_quantiles || '0.75,0.90' }}
      RANK_METRICS: ${{ github.event.inputs.rank_metrics || 'utility' }}
      LAMBDA_TAIL: ${{ github.event.inputs.lambda_tail || '0.05' }}
      REBUILD_ALL: ${{ github.event.inputs.rebuild_all || 'false' }}

      # Gate 모드 기본
      GATE_MODES: "none,tail,utility,tail_utility"
      TAU_GAMMA: "0.0"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install pandas numpy yfinance pyarrow scikit-learn joblib
          fi

      - name: Ensure folders
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/raw data/features data/labels data/signals data/meta data/_backup app scripts

      - name: Build TAG + LABEL_SPEC (safe for cache keys)
        shell: bash
        run: |
          set -euo pipefail

          # PT=0.10 -> PT10, SL=-0.10 -> SL10
          PT100="$(python - <<'PY'
import os
pt=float(os.environ["PROFIT_TARGET"])
print(int(round(pt*100)))
PY
)"
          SL100="$(python - <<'PY'
import os
sl=float(os.environ["STOP_LEVEL"])
print(int(round(abs(sl)*100)))
PY
)"

          TAG="pt${PT100}_h${MAX_DAYS}_sl${SL100}_ex${MAX_EXTEND_DAYS}"

          # ✅ cache key에 콤마 들어가면 터짐 -> LABEL_SPEC_SAFE는 콤마 없이
          echo "TAG=$TAG" >> $GITHUB_ENV
          echo "LABEL_SPEC_SAFE=$TAG" >> $GITHUB_ENV

          echo "[INFO] PROFIT_TARGET=$PROFIT_TARGET MAX_DAYS=$MAX_DAYS STOP_LEVEL=$STOP_LEVEL MAX_EXTEND_DAYS=$MAX_EXTEND_DAYS"
          echo "[INFO] TAG=$TAG"

      # ---- PRICES cache
      - name: Restore cache (PRICES)
        uses: actions/cache@v4
        with:
          path: |
            data/raw/prices.parquet
            data/raw/prices.csv
            data/raw/prices_meta.json
          key: mumumeme-prices-v3-${{ runner.os }}-${{ hashFiles('data/universe.csv','scripts/universe.py','scripts/fetch_prices.py') }}
          restore-keys: |
            mumumeme-prices-v3-${{ runner.os }}-

      # ---- FEATURES cache
      - name: Restore cache (FEATURES_MODEL)
        uses: actions/cache@v4
        with:
          path: |
            data/features/features_model.parquet
            data/features/features_model.csv
          key: mumumeme-features-v3-${{ runner.os }}-${{ hashFiles('data/universe.csv','scripts/universe.py','scripts/fetch_prices.py','scripts/build_features.py','scripts/build_ev_features.py','scripts/merge_incremental_table.py') }}
          restore-keys: |
            mumumeme-features-v3-${{ runner.os }}-

      # ---- MODEL cache
      - name: Restore cache (MODEL)
        uses: actions/cache@v4
        with:
          path: |
            data/labels/labels_model.parquet
            data/labels/labels_model.csv
            app/model.pkl
            app/scaler.pkl
            data/meta/build_fingerprint.json
          key: mumumeme-model-v3-${{ runner.os }}-${{ env.LABEL_SPEC_SAFE }}-${{ hashFiles('scripts/build_labels.py','scripts/train_model.py','scripts/check_rebuild.py','scripts/merge_incremental_table.py') }}
          restore-keys: |
            mumumeme-model-v3-${{ runner.os }}-${{ env.LABEL_SPEC_SAFE }}-
            mumumeme-model-v3-${{ runner.os }}-

      - name: Fingerprint check (partial rebuild flags)
        shell: bash
        run: |
          set -euo pipefail
          out="$(python scripts/check_rebuild.py \
            --fingerprint-path data/meta/build_fingerprint.json \
            --label-spec "${LABEL_SPEC_SAFE}" \
            --features-files "scripts/build_features.py,scripts/build_ev_features.py,scripts/merge_incremental_table.py" \
            --labels-files "scripts/build_labels.py,scripts/train_model.py,scripts/merge_incremental_table.py" \
            --universe-files "scripts/universe.py,data/universe.csv" \
            --strategy-files "scripts/predict_gate.py,scripts/simulate_single_position_engine.py,scripts/gate_grid_lib.sh,scripts/run_grid_workflow.sh,scripts/summarize_sim_trades.py,scripts/aggregate_gate_grid.py,scripts/build_tau_labels.py" \
          )"
          echo "$out"
          echo "$out" | tail -n 4 >> $GITHUB_ENV

      - name: Prepare required files (daily incremental)
        shell: bash
        run: |
          set -euo pipefail

          # ✅ unbound 방지(혹시라도 env가 비면 기본값)
          : "${PROFIT_TARGET:=0.10}"
          : "${MAX_DAYS:=40}"
          : "${STOP_LEVEL:=-0.10}"
          : "${MAX_EXTEND_DAYS:=30}"
          : "${REBUILD_ALL:=false}"

          if [ "${REBUILD_ALL}" = "true" ]; then
            REBUILD_PRICES="1"; REBUILD_FEATURES="1"; REBUILD_MODEL="1"; CLEAR_SIGNALS="1"
          fi

          if [ "${CLEAR_SIGNALS:-0}" = "1" ]; then
            rm -rf data/signals/* || true
          fi

          # upstream 삭제
          if [ "${REBUILD_PRICES:-0}" = "1" ]; then
            rm -f data/raw/prices.parquet data/raw/prices.csv data/raw/prices_meta.json || true
            rm -f data/features/features_model.parquet data/features/features_model.csv || true
            rm -f data/labels/labels_model.parquet data/labels/labels_model.csv || true
            rm -f app/model.pkl app/scaler.pkl || true
          elif [ "${REBUILD_FEATURES:-0}" = "1" ]; then
            rm -f data/features/features_model.parquet data/features/features_model.csv || true
            rm -f data/labels/labels_model.parquet data/labels/labels_model.csv || true
            rm -f app/model.pkl app/scaler.pkl || true
          elif [ "${REBUILD_MODEL:-0}" = "1" ]; then
            rm -f data/labels/labels_model.parquet data/labels/labels_model.csv || true
            rm -f app/model.pkl app/scaler.pkl || true
          fi

          # 1) prices
          python scripts/universe.py

          if [ ! -f data/raw/prices.parquet ] && [ ! -f data/raw/prices.csv ]; then
            # ✅ 10년 유효 + 워밍업(기본 11년)으로 제한하고 싶으면 fetch_prices.py에 --max-years 옵션 추가한 버전 사용
            # python scripts/fetch_prices.py --include-extra --reset --force-full --max-years 11
            python scripts/fetch_prices.py --include-extra --reset --force-full
          else
            # python scripts/fetch_prices.py --include-extra --lookback-days 10 --max-years 11
            python scripts/fetch_prices.py --include-extra --lookback-days 10
          fi

          # 2) START_DATE 자동 산출 (features 마지막 날짜 기준)
          RECOMPUTE_DAYS=20
          START_DATE=""
          if [ -f data/features/features_model.parquet ] || [ -f data/features/features_model.csv ]; then
            START_DATE="$(python - <<'PY'
import pandas as pd
from pathlib import Path
p=Path("data/features/features_model.parquet")
c=Path("data/features/features_model.csv")
df=None
if p.exists():
  df=pd.read_parquet(p)
elif c.exists():
  df=pd.read_csv(c)
if df is None or len(df)==0 or "Date" not in df.columns:
  print("")
else:
  d=pd.to_datetime(df["Date"], errors="coerce").dropna()
  if len(d)==0:
    print("")
  else:
    last=d.max()
    start=last - pd.Timedelta(days=20)
    print(start.strftime("%Y-%m-%d"))
PY
)"
          fi
          echo "[INFO] START_DATE=$START_DATE"

          # 3) features: backup -> build recent -> merge
          if [ -n "$START_DATE" ] && { [ -f data/features/features_model.parquet ] || [ -f data/features/features_model.csv ]; }; then
            rm -rf data/_backup/features || true
            mkdir -p data/_backup/features
            [ -f data/features/features_model.parquet ] && cp data/features/features_model.parquet data/_backup/features/old.parquet || true
            [ -f data/features/features_model.csv ] && cp data/features/features_model.csv data/_backup/features/old.csv || true

            python scripts/build_features.py --start-date "$START_DATE" --max-window 260 --buffer-days 40

            rm -rf data/_backup/features/new || true
            mkdir -p data/_backup/features/new
            [ -f data/features/features_model.parquet ] && mv data/features/features_model.parquet data/_backup/features/new/new.parquet || true
            [ -f data/features/features_model.csv ] && mv data/features/features_model.csv data/_backup/features/new/new.csv || true

            python scripts/merge_incremental_table.py \
              --old-parq data/_backup/features/old.parquet \
              --old-csv data/_backup/features/old.csv \
              --new-parq data/_backup/features/new/new.parquet \
              --new-csv data/_backup/features/new/new.csv \
              --out-parq data/features/features_model.parquet \
              --out-csv data/features/features_model.csv \
              --cut-date "$START_DATE"
          else
            python scripts/build_features.py
          fi

          # 4) labels (p_success용)
          if [ -n "$START_DATE" ] && { [ -f data/labels/labels_model.parquet ] || [ -f data/labels/labels_model.csv ]; }; then
            rm -rf data/_backup/labels || true
            mkdir -p data/_backup/labels
            [ -f data/labels/labels_model.parquet ] && cp data/labels/labels_model.parquet data/_backup/labels/old.parquet || true
            [ -f data/labels/labels_model.csv ] && cp data/labels/labels_model.csv data/_backup/labels/old.csv || true

            python scripts/build_labels.py \
              --start-date "$START_DATE" \
              --profit-target "$PROFIT_TARGET" \
              --max-days "$MAX_DAYS" \
              --stop-level "$STOP_LEVEL" \
              --buffer-days 120

            rm -rf data/_backup/labels/new || true
            mkdir -p data/_backup/labels/new
            [ -f data/labels/labels_model.parquet ] && mv data/labels/labels_model.parquet data/_backup/labels/new/new.parquet || true
            [ -f data/labels/labels_model.csv ] && mv data/labels/labels_model.csv data/_backup/labels/new/new.csv || true

            python scripts/merge_incremental_table.py \
              --old-parq data/_backup/labels/old.parquet \
              --old-csv data/_backup/labels/old.csv \
              --new-parq data/_backup/labels/new/new.parquet \
              --new-csv data/_backup/labels/new/new.csv \
              --out-parq data/labels/labels_model.parquet \
              --out-csv data/labels/labels_model.csv \
              --cut-date "$START_DATE"
          else
            python scripts/build_labels.py \
              --profit-target "$PROFIT_TARGET" \
              --max-days "$MAX_DAYS" \
              --stop-level "$STOP_LEVEL"
          fi

          # 5) p_success model
          if [ ! -f app/model.pkl ] || [ ! -f app/scaler.pkl ]; then
            python scripts/train_model.py
          else
            echo "[INFO] model exists -> reuse"
          fi

          # 6) tau labels (✅ stop/max_extend 넘김 + unbound 방지)
          if [ ! -f data/labels/labels_tau.parquet ] && [ ! -f data/labels/labels_tau.csv ]; then
            python scripts/build_tau_labels.py \
              --profit-target "$PROFIT_TARGET" \
              --max-days "$MAX_DAYS" \
              --stop-level "$STOP_LEVEL" \
              --max-extend-days "$MAX_EXTEND_DAYS" \
              --k1 10 \
              --k2 20
          fi

      - name: Run gate grid (predict + simulate + summarize)
        shell: bash
        run: |
          set -euo pipefail
          chmod +x scripts/run_grid_workflow.sh || true
          bash scripts/run_grid_workflow.sh

      - name: Aggregate summaries
        shell: bash
        run: |
          set -euo pipefail
          python scripts/aggregate_gate_grid.py \
            --signals-dir "data/signals" \
            --out-aggregate "data/signals/gate_grid_aggregate.csv" \
            --out-top "data/signals/gate_grid_top_by_recent10y.csv" \
            --pattern "gate_summary_*.csv" \
            --topn 30

      - name: Upload artifacts (signals + meta)
        uses: actions/upload-artifact@v4
        with:
          name: gate-grid-results
          path: |
            data/meta/build_fingerprint.json
            data/signals/gate_summary_*.csv
            data/signals/gate_grid_aggregate.csv
            data/signals/gate_grid_top_by_recent10y.csv
            data/signals/picks_*.csv
            data/signals/picks_meta_*.json
            data/signals/sim_engine_trades_*.parquet
            data/signals/sim_engine_curve_*.parquet
            data/labels/labels_tau.parquet
            data/labels/labels_tau.csv
          if-no-files-found: error